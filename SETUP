Prerequisites : 
The system has more than 4 cores and 8 gb memory
Ubuntu version 20.04 or greater
System has python 3 install which can be done by
~sudo snap install python3
System has docker installed
~sudo apt-get update
  sudo apt-get install ca-certificates curl gnupg

~sudo install -m 0755 -d /etc/apt/keyrings
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /etc/apt/keyrings/docker.gpg
sudo chmod a+r /etc/apt/keyrings/docker.gpg

~echo \
  "deb [arch="$(dpkg --print-architecture)" signed-by=/etc/apt/keyrings/docker.gpg] https://download.docker.com/linux/ubuntu \
  "$(. /etc/os-release && echo "$VERSION_CODENAME")" stable" | \
  sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

~sudo apt-get update
~sudo apt-get install docker-ce docker-ce-cli containerd.io docker-buildx-plugin docker-compose-plugin

STEP 1:Creating a Kubernetes Cluster(Using Microk8s)
(MicroK8s is a lightweight Kubernetes distribution developed by Canonical, the company behind Ubuntu Linux.)
~sudo snap install microk8s --classic --channel=1.24/stable

Install the necessary addons for Kubeflow
~microk8s enable dns hostpath-storage ingress metallb:10.64.140.43-10.64.140.49

Give super-user permissions to Microk8s
~sudo usermod -a -G microk8s $USER
newgrp microk8s
~sudo chown -f -R $USER ~/.kube

STEP 2:INSTALL JUJU
(Juju is an open-source application modeling and deployment tool developed by Canonical, the company behind Ubuntu Linux. It's designed to simplify the process of deploying, managing, and scaling complex applications across various cloud platforms, virtualization technologies, and bare-metal servers.)
~ sudo snap install juju --classic --channel=2.9/stable

Run the following command to deploy a Juju controller to the Kubernetes we set up with MicroK8s:
~juju bootstrap microk8s

Add a model for Kubeflow to the controller:
~juju add-model kubeflow

Microk8s uses inotify to interact with the filesystem, and in kubeflow sometimes the default inotify limits are exceeded.
~sudo sysctl fs.inotify.max_user_instances=1280
~sudo sysctl fs.inotify.max_user_watches=655360

STEP 3: Install Kubeflow using JUJU
~juju deploy kubeflow --trust  --channel=1.7/stable

STEP 4: Configure Dashboard Access
~microk8s kubectl -n kubeflow get svc istio-ingressgateway-workload -o jsonpath='{.status.loadBalancer.ingress[0].ip}'

Configure Authentication and Authorisation
~juju config dex-auth public-url=http://10.64.140.43.nip.io
~juju config oidc-gatekeeper public-url=http://10.64.140.43.nip.io
~juju config dex-auth static-username=admin
~juju config dex-auth static-password=admin

Verify the installation by accessing the url on browser
http://10.64.140.43.nip.io

STEP 5: Install KServe

 Install Knative
~kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.11.0/serving-crds.yaml

~kubectl apply -f https://github.com/knative/serving/releases/download/knative-v1.11.0/serving-core.yaml

INSTALL Kourier Controller(Networking Layer)
~kubectl apply -f https://github.com/knative/net-kourier/releases/download/knative-v1.11.1/kourier.yaml

Configure Knative Serving
~kubectl patch configmap/config-network \
  --namespace knative-serving \
  --type merge \
  --patch '{"data":{"ingress-class":"kourier.ingress.networking.knative.dev"}}'

Fetch the External IP address or CNAME by running the command:
~kubectl --namespace kourier-system get service kourier

INSTALL Cert Manager
~kubectl apply -f https://github.com/cert-manager/cert-manager/releases/download/v1.12.0/cert-manager.yaml
Install KServe
~kubectl apply -f https://github.com/kserve/kserve/releases/download/v0.10.0/kserve.yaml
Install KServe Built-in ClusterServingRuntimes
~kubectl apply -f https://github.com/kserve/kserve/releases/download/v0.10.0/kserve-runtimes.yaml


Python script to train a model on a dataset
Github clone https://github.com/adityaoncloud/SVC_model.git
#Here’s the actual code 
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score, confusion_matrix
import seaborn as sns
import joblib

# Load the Iris dataset
data = load_iris()
X = data.data
y = data.target

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Support Vector Machine (SVM) classifier
model = SVC()

# Train the model on the training data
model.fit(X_train, y_train)

# Make predictions on the test data
y_pred = model.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.4f}")

# Create a confusion matrix
conf_matrix = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(6, 4))
sns.heatmap(conf_matrix, annot=True, cmap="Blues", fmt="d", xticklabels=data.target_names, yticklabels=data.target_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()
joblib.dump(model, 'trained_model.pkl')

Then run the following code and trained_model.pkl script will be formed.

Then run the following python code (inference_script.py) for deploying the trained model as serverless deployment using kserve on kubeflow 

import numpy as np
from flask import Flask, request, jsonify
from sklearn.svm import SVC
import joblib

app = Flask(__name__)

# Load the trained SVM model
model = joblib.load('trained_model.pkl')

@app.route('/predict', methods=['POST'])
def predict():
	data = request.json['data']
	data = np.array(data).reshape(1, -1)
	prediction = model.predict(data)
	return jsonify({'prediction': prediction.tolist()})

if __name__ == '__main__':
	app.run(host='0.0.0.0', port=8080)


Create a dockerfile for the following and then push it to a central repository Dockerhub so we can pull it into our cluster from a remote server.



Here’s the Dockerfile code
# Use a base image with Python pre-installed
FROM python:3.8

# Set the working directory inside the container
                   WORKDIR /app

# Copy the inference script and trained model into the container
COPY inference_script.py /app/
COPY trained_model.pkl /app/

# Install dependencies
RUN pip install Flask numpy scikit-learn matplotlib seaborn joblib

# Expose the port your application will listen on
EXPOSE 8080

# Define the command to run when the container starts
CMD ["python", "inference_script.py"]

Build the image using
~docker build -t <name> .

Then using kubectl create your namespace for kserve resource deployment

~sudo microk8s kubectl create namespace 

Create an inferenceservice.yaml for the model and apply it to the cluster


Here’s the yaml file

apiVersion: "serving.kserve.io/v1beta1"
kind: "InferenceService"
metadata:
  name: "my-model"
spec:
  predictor:
	containers:
  	- image: "aditya252003/svc:v2"

~                                                                          	 
~                                                     	

Then apply the inferenceService Definition to Kubernetes cluster to the directory you saved the yaml file.

~ kubectl apply -f inferenceservice.yaml

To monitor the deployment you can use the following command.

~ kubectl get inferenceservices my-model-service

To track the performance of the deployed model using Kserve’s Rest api use the following python script 

#This is a Python script to test the deployed model's performance using the Kserve REST API.







import requests
import json

# Define the endpoint URL of the deployed model
endpoint_url = "http://my-model-predictor-default.default.10.64.140.43.nip.io"

#Enter your endpoint url

# Sample input data for testing
sample_input = {
	"data": ['sample data expected by the model']
}

# Send a POST request to the endpoint with the sample input
response = requests.post(endpoint_url, json=sample_input)

# Check if the request was successful
if response.status_code == 200:
	# Parse the response JSON
	result = response.json()

	# Assuming the model's response contains a 'predictions' key
	predictions = result.get('predictions', [])

	# Print the predictions
	print("Predictions:", predictions)
else:
	print("Request failed with status code:", response.status_code)
	print("Response content:", response.content.decode('utf-8'))




















